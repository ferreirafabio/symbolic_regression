#!/bin/bash -x
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=12
#SBATCH --job-name=gpr
#SBATCH -D .
#SBATCH --threads-per-core=1
#SBATCH --time=1:00:00
#SBATCH --output=/home/frankej/experiments/symreg/output/mpi-out.%j
#SBATCH --error=/home/frankej/experiments/symreg/error/mpi-err.%j


CODE_DIR="/home/frankej/workspace/ScalingSymbolicRegression"



export MASTER_PORT=20073
export NUM_NODES=$SLURM_JOB_NUM_NODES
export GPUS_PER_NODE=4
export NUM_GPUS_PER_NODE=4
export NUM_GPUS=$((NUM_GPUS_PER_NODE*SLURM_NNODES))
# export NCCL_DEBUG=INFO

echo "INFO: number of nodes: $NUM_NODES"
echo "INFO: number of gpus per node: $NUM_GPUS_PER_NODE"
echo "INFO: number of gpus: $NUM_GPUS"
echo "INFO: number of nnudes: $SLURM_NNODES"


ACCELERATE_CONFIG_FILE="${CODE_DIR}/scripts/accelerate.yaml"

cat << EOT > $ACCELERATE_CONFIG_FILE
# WARNING: do not edit this file as this is an slurm-auto-generated file
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: MULTI_GPU
main_process_ip: $MASTER_ADDR
main_process_port: $MASTER_PORT
main_training_function: main
num_machines: $SLURM_NNODES
num_processes: $NUM_GPUS
use_cpu: false
EOT


cuda12.1

echo "SLURM_PARTITION=$SLURM_NODELIST"

echo "JUWELS Job"
source /home/frankej/workspace/ScalingSymbolicRegression/venv/bin/activate

PYTHON_SCRIPT=/home/frankej/workspace/ScalingSymbolicRegression/train_gpr.py

for (( i=0; i<$NUM_NODES; i++ )); do
    # Build the command
    cmd="srun -lN1 -n1 -r $i accelerate launch \
        --config_file \$ACCELERATE_CONFIG_FILE \
        --rdzv_conf \"rdzv_backend=c10d,rdzv_endpoint=\$MASTER_ADDR:\$MASTER_PORT\" \
        --main_process_ip \$MASTER_ADDR \
        --main_process_port \$MASTER_PORT \
        --machine_rank $i \
         \$PYTHON_SCRIPT -c juwels_config.yaml \$@"

    # Execute the command
    if [[ $i -lt $(($NUM_NODES - 1)) ]]; then
        # Execute in the background for all but the last task
        eval $cmd &
    else
        # Execute the last task in the foreground
        eval $cmd
    fi
done

