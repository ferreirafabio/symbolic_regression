experiment:
  experiments_base_dir: /p/scratch/laionize/franke5/experiments/
  project_name: gpr_project
  session_name: design_model
  experiment_name: test_setup


train:
  seed: 1
  clip_value: 0.1
  val_interval: 500
  log_interval: 100
  log_param_interval: 500
  save_interval: 10000
  max_steps: 10000
  max_epochs: null


optim:
  optimizer: AdamW
  lr: 0.002
  weight_decay: 0.1
  betas:
    - 0.9
    - 0.98
  eps: 1.0e-09
  normalization_regularization: False
  bias_regularization: False
  scheduler:
    num_warmup_steps: 400 #${eval:0.01 * ${trainer.max_steps}}
    num_training_steps: ${train.max_steps}
    decay_factor: 0.01
    schedule: "cosine" # "cosine" or "linear"
  cpr_config:
    kappa_init_param: 400
    kappa_init_method: 'warm_start'
    reg_function: 'l2'
    kappa_update: 1.0


accelerate:
  mixed_precision: bf16


model:
    trg_vocab_size: 1
    seq_vocab_size: 1
    model_dim: 128
    max_len: 500
    num_head: 2
    n_layers: 6
    ff_factor: 4
    activation: relu
    glu: false
    softmax_scale: True
    pos_embedding: True
    rel_pos_enc: False
    add_unit_offset: False
    rms_norm: False
    resi_dropout: 0.1
    attn_dropout: 0.1
    ln_eps: 1e-5
    use_bias: True
    learn_ln: True
    last_zero: False
    initializer_range: 0.02


dataloader:
    generator_type: PolynomialGenerator
    generator:
        num_variables: 4
        max_terms: 4
        num_realizations: 200
        allowed_operations: ["+", "-", "*", "/", "cos", "log", "sin", "exp"]
        real_const_decimal_places: 3
        real_constants_min: -10
        real_constants_max: 10
        max_const_exponent: 3
        use_math_constants: False
        sample_interval: [-10, 10]
        keep_graph: False
        keep_data: False
        max_powers: 3
        seed: 1
    train_samples: 1000
    valid_samples: 500
    batch_size: 64
    num_workers: 8
    data_dir: "./data"
