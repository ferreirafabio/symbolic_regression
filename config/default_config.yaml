experiment:
  experiments_base_dir: ".experiments/"
  project_name: gpr_project
  session_name: design_model
  experiment_name: test_inference


train:
  seed: 1
  clip_value: 1.0
  val_interval: 100
  log_interval: 100
  log_param_interval: 100
  save_interval: 1000
  max_steps: 4000
  max_epochs: null


optim:
  optimizer: AdamCPR
  lr: 0.0001
  weight_decay: 0.1
  betas:
    - 0.9
    - 0.98
  eps: 1.0e-09
  normalization_regularization: False
  bias_regularization: False
  scheduler:
    num_warmup_steps: 400 #${eval:0.01 * ${trainer.max_steps}}
    num_training_steps: ${train.max_steps}
    decay_factor: 0.01
    schedule: "cosine" # "cosine" or "linear"
  cpr_config:
    kappa_init_param: 1000
    kappa_init_method: 'inflection_point'
    reg_function: 'l2'
    kappa_update: 1.0


accelerate:
  mixed_precision: bf16


model:
    trg_vocab_size: 1
    seq_vocab_size: 1
    max_var_pos: 5
    model_dim: 128
    max_len: 200
    num_head: 2
    n_layers_enc: 2
    n_layers_dec: 4
    ff_factor: 4
    activation: silu
    glu: false
    softmax_scale: True
    pos_embedding: True
    rel_pos_enc: False
    add_unit_offset: False
    rms_norm: False
    resi_dropout: 0.1
    attn_dropout: 0.1
    ln_eps: 1e-5
    use_bias: True
    learn_ln: True
    last_zero: False
    initializer_range: 0.02



dataloader:
    generator_type: PolynomialGenerator
    generator:
        num_variables: 3
        max_terms: 3
        num_realizations: 100
        allowed_operations: ["+", "-"]
        real_const_decimal_places: 2
        real_constants_min: -10
        real_constants_max: 10
        max_const_exponent: 0
        use_math_constants: False
        sample_interval: [-10, 10]
        keep_graph: False
        keep_data: False
        max_powers: 1
        seed: 1
    train_samples: 50000
    valid_samples: 500
    batch_size: 64
    num_workers: 8
    data_dir: "./data"
