experiment:
  experiments_base_dir: ".experiments/"
  project_name: gpr_project
  session_name: design_model
  experiment_name: test_setup


train:
  seed: 1
  clip_value: 1.0
  val_interval: 100
  log_interval: 100
  log_param_interval: 500
  save_interval: 10000
  max_steps: 20000
  max_epochs: null


optim:
  optimizer: AdamW
  lr: 0.0001
  weight_decay: 0.1
  betas:
    - 0.9
    - 0.98
  eps: 1.0e-09
  normalization_regularization: False
  bias_regularization: False
  scheduler:
    num_warmup_steps: 400 #${eval:0.01 * ${trainer.max_steps}}
    num_training_steps: ${train.max_steps}
    decay_factor: 0.01
    schedule: "cosine" # "cosine" or "linear"
  cpr_config:
    kappa_init_param: 400
    kappa_init_method: 'warm_start'
    reg_function: 'l2'
    kappa_update: 1.0


accelerate:
  mixed_precision: bf16


model:
    trg_vocab_size: 1
    seq_vocab_size: 1
    model_dim: 256
    max_len: 400
    num_head: 4
    n_layers: 4
    ff_factor: 4
    activation: silu
    glu: false
    softmax_scale: True
    pos_embedding: True
    rel_pos_enc: False
    add_unit_offset: False
    rms_norm: False
    resi_dropout: 0.1
    attn_dropout: 0.1
    ln_eps: 1e-5
    use_bias: True
    learn_ln: True
    last_zero: False
    initializer_range: 0.02



dataloader:
    generator_type: PolynomialGenerator
    generator:
        num_nodes: 2
        num_edges: 2
        max_terms: 4
        num_realizations: 100
        allowed_operations: ["+", "-", "*", "/", "cos", "log", "sin", "exp"]
        real_numbers_realizations: False
        keep_graph: False
        keep_data: False
        real_numbers_variables: False
        num_variables: 2
        max_powers: 1
        seed: 1
    train_samples: 50000
    valid_samples: 5000
    batch_size: 64
    num_workers: 8
    data_dir: "./data"
