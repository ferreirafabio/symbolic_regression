






experiment:
  experiments_base_dir: /home/joerg/workspace/experiments/gpt_model
  project_name: GPT
  session_name: test_2
  experiment_name: ts0_conform_RMAtrue_zeroTrue


train:
  seed: 1
  clip_value: 0.1
  val_interval: 500
  log_interval: 100
  log_param_interval: 500
  save_interval: 10000
  max_steps: 10000
  max_epochs: null


optim:
  optimizer: AdamW
  lr: 0.002
  weight_decay: 0.1
  betas:
    - 0.9
    - 0.99
  eps: 1.0e-09
  normalization_regularization: False
  bias_regularization: False
  scheduler:
    num_warmup_steps: 400 #${eval:0.01 * ${trainer.max_steps}}
    num_training_steps: ${train.max_steps}
    decay_factor: 0.1
    schedule: "cosine" # "cosine" or "linear"
  cpr_config:
    kappa_init_param: 400
    kappa_init_method: 'warm_start'
    reg_function: 'l2'
    kappa_update: 1.0


accelerate:
  mixed_precision: bf16


model:
    trg_vocab_size: 200
    seq_vocab_size: 1000
    model_dim: 64
    max_len: 500
    num_head: 2
    n_layers: 6
    ff_factor: 4
    activation: relu
    glu: false
    softmax_scale: True
    pos_embedding: True
    rel_pos_enc: False
    add_unit_offset: False
    rms_norm: False
    resi_dropout: 0.1
    attn_dropout: 0.1
    ln_eps: 1e-5
    use_bias: True
    learn_ln: True
    last_zero: False
    initializer_range: 0.02


data_simple:
  num_variables: 4
  max_powers: 2
  max_terms: 4
  num_realisations: 10
  val_samples: 100
  batch_size: 8
  real_numbers_variables: False
  seed: 1