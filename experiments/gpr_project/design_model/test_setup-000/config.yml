accelerate:
  mixed_precision: fp16
dataloader:
  batch_size: 64
  generator:
    allowed_operations:
    - +
    - '-'
    - '*'
    - /
    - cos
    - log
    - sin
    - exp
    keep_data: false
    keep_graph: false
    kmax: 5
    max_powers: 1
    max_terms: 4
    num_realizations: 100
    num_variables: 2
    real_numbers_realizations: false
    real_numbers_variables: false
    seed: 1
  num_workers: 8
  test_samples: 200
  val_samples: 500
experiment:
  experiment_name: test_setup
  experiments_base_dir: /work/dlclarge2/ferreira-symreg/ScalingSymbolicRegression/experiments
  project_name: gpr_project
  session_name: design_model
model:
  activation: silu
  add_unit_offset: false
  attn_dropout: 0.1
  ff_factor: 4
  glu: false
  initializer_range: 0.02
  last_zero: false
  learn_ln: true
  ln_eps: 1.0e-05
  max_len: 400
  model_dim: 256
  n_layers_dec: 6
  n_layers_enc: 4
  num_head: 4
  pos_embedding: true
  rel_pos_enc: false
  resi_dropout: 0.1
  rms_norm: false
  seq_vocab_size: 1
  softmax_scale: true
  trg_vocab_size: 1
  use_bias: true
optim:
  betas:
  - 0.9
  - 0.98
  bias_regularization: false
  cpr_config:
    kappa_init_method: warm_start
    kappa_init_param: 400
    kappa_update: 1.0
    reg_function: l2
  eps: 1.0e-09
  lr: 0.0001
  normalization_regularization: false
  optimizer: AdamW
  scheduler:
    decay_factor: 0.01
    num_training_steps: 20000
    num_warmup_steps: 400
    schedule: cosine
  weight_decay: 0.1
train:
  clip_value: 1.0
  log_interval: 100
  log_param_interval: 500
  max_epochs: null
  max_steps: 20000
  save_interval: 10000
  seed: 1
  val_interval: 500
