


sbatch --nodes=4 --account=projectnucleus --partition=develbooster --time=1:00:00 slurm_launch_juwels.sh "experiment.session_name=design_model
experiment.experiment_name=test_setup_1024-12_bs4x4x8_rel200
model.model_dim=1024
model.num_head=8
model.n_layers=12
dataloader.generator.num_realizations=100
dataloader.val_samples=500
dataloader.batch_size=16
optim.lr=0.0003"


sbatch --nodes=4 --account=projectnucleus --partition=booster --time=10:00:00 slurm_launch_juwels.sh "experiment.session_name=first_hpo_1
experiment.experiment_name=setup_1024-12_bs4x4x16_rel200_lr0003
model.model_dim=1024
model.num_head=8
model.n_layers=12
dataloader.num_realisations=200
dataloader.val_samples=500
dataloader.batch_size=16
optim.lr=0.0003"

sbatch --nodes=4 --account=projectnucleus --partition=booster --time=10:00:00 slurm_launch_juwels.sh "experiment.session_name=first_hpo_1
experiment.experiment_name=setup_1024-12_bs4x4x16_rel200_lr001
model.model_dim=1024
model.num_head=8
model.n_layers=12
dataloader.num_realisations=200
dataloader.val_samples=500
dataloader.batch_size=16
optim.lr=0.001"



sbatch --nodes=4 --account=projectnucleus --partition=booster --time=10:00:00 slurm_launch_juwels.sh "experiment.session_name=first_hpo_1
experiment.experiment_name=setup_1024-12_bs4x4x16_rel200_lr001
model.model_dim=1024
model.num_head=8
model.n_layers=12
dataloader.num_realisations=200
dataloader.val_samples=500
dataloader.batch_size=16
optim.lr=0.001"



# GET COMPUTE NODE
srun --cpu_bind=none,v --accel-bind=gn --threads-per-core=1 --nodes=1 -A cstdl --partition=develbooster --gres gpu:4 --pty --time=01:00:00 /bin/bash

# SETUP COMPUTE NODE

CODE_DIR="/p/project/projectnucleus/franke5/ScalingSymbolicRegression"

export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}

export NCCL_IB_TIMEOUT=50
export UCX_RC_TIMEOUT=4s
export NCCL_IB_RETRY_CNT=10
export NCCL_SOCKET_IFNAME=ib0


export CUDA_VISIBLE_DEVICES="0,1,2,3"
# export CUDA_LAUNCH_BLOCKING=0

MASTER_ADDR=`scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1`
# Allow communication over InfiniBand cells.

export MASTER_ADDR="${MASTER_ADDR}i"

export MASTER_PORT=20073
export NUM_NODES=$SLURM_JOB_NUM_NODES
export GPUS_PER_NODE=4
export NUM_GPUS_PER_NODE=4
export NUM_GPUS=$((NUM_GPUS_PER_NODE*SLURM_NNODES))
# export NCCL_DEBUG=INFO

echo "INFO: number of nodes: $NUM_NODES"
echo "INFO: number of gpus per node: $NUM_GPUS_PER_NODE"
echo "INFO: number of gpus: $NUM_GPUS"
echo "INFO: number of nnudes: $SLURM_NNODES"


ACCELERATE_CONFIG_FILE="${CODE_DIR}/scripts/accelerate.yaml"

cat << EOT > $ACCELERATE_CONFIG_FILE
# WARNING: do not edit this file as this is an slurm-auto-generated file
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: MULTI_GPU
main_process_ip: $MASTER_ADDR
main_process_port: $MASTER_PORT
main_training_function: main
num_machines: $SLURM_NNODES
num_processes: $NUM_GPUS
use_cpu: false
EOT


ml Stages/2024
ml CUDA/12
ml GCC/12.3.0
ml Python/3.11.3

echo "SLURM_PARTITION=$SLURM_NODELIST"

if echo "$SLURM_NODELIST" | grep -q "jrc"; then
    echo "JURECA Job"
    source /p/scratch/laionize/franke5/pt22r/bin/activate
else
    echo "JUWELS Job"
    source /p/scratch/projectnucleus/franke5/pt22/bin/activate
fi

PYTHON_SCRIPT=/p/project/projectnucleus/franke5/ScalingSymbolicRegression/train_gpr.py



# RUN GENERATOR
python gpr/data/data_creator.py -c juwels_config dataloader.generator.num_realizations=500

# RUN TRAINING
accelerate launch --config_file $ACCELERATE_CONFIG_FILE --machine_rank 0 train_gpr.py -c juwels_config
